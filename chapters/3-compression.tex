\chapter{Hyperspectral Image Compression}

The Consultative Committee for Space Data Systems (CCSDS) is a collaborative group working on the development of standards and algorithms for space applications. CCSDS Data Compression Working Group has released several algorithms for the compression of multi- and hyperspectral images at different stages during the compression process \cite{hernandez-cabroneroCCSDS1230B2LowComplexity2021}. In 2012 the committee released the CCSDS-123.0-B-1 specification for lossless compression, hereby referred to as \textit{issue 1}. This was in 2019 followed by the CCSDS-123.0-B-2, referred to as \textit{issue 2}, introducing support for near-lossless compression. The following section gives a brief overview of the issue 2 algorithm, and its differences with issue 1. For a detailed walk-through see the CCSDS specification \fullcite{LowComplexityLosslessNearLossless2019} \cite{LowComplexityLosslessNearLossless2019}.

\section{Algorithm Overview}

\begin{figure}[h]
	\begin{center}
		\includegraphics[width=0.7\textwidth]{figures/block-diagram.png}
	\end{center}
	\caption{Block diagram of the issue 1 and issue 2 compression algorithms.}
	\label{fig:block-diagram}
\end{figure}

\autoref{fig:block-diagram} shows a block diagram of the issue 1 and issue 2 compression algorithms. The raw image samples are processed in a single pass, allowing fast implementations and minimal temporary storage. Samples are fed to the predictor stage, where sample values are predicted with an adaptive linear prediction method, using a small neighborhood of spatially and spectrally adjacent pixels \cite{chatziantoniouHighPerformanceRTLImplementation}. The results of the predictor are passed on to an encoder stage in which the image data is encoded into its compressed format. Each of the stages in the algorithm are controlled with a set of parameters, which are needed by the decoder. In the final stage these parameters are packed into a header which is shipped together with the compressed image.

As mentioned, the main new feature of issue 2 is the option for near-lossless compression. The notion of near-lossless, as compared to lossy compression, is that the user is provided with fine tuned control over the per pixel error limit through setting an absolute and a relative limit. The standard also allows band-dependant error limits, allowing higher compression of less important spectral bands \cite{hernandez-cabroneroCCSDS1230B2LowComplexity2021}. This is useful in the context of the HYPSO project, where the outer spectral bands tend to contain more noise \cite{bakkenHYPSO1CubeSatFirst2023}.

Another important new feature of issue 2 is the addition of a hybrid-entropy encoder, in addition to the sample-adaptive and block-adaptive encoders of issue 1. The work done by \citeauthor{chatziantoniouHighPerformanceRTLImplementation} suggests an architecture, written in portable VHDL, for such a hybrid-entropy encoder achieving a throughput of 341 $Msamples/s$ \cite{chatziantoniouHighPerformanceRTLImplementation}. The encoder implemented by \citeauthor{basconesRealTimeFPGAImplementation2022} achieve similar speeds and outperforming that of \citeauthor{chatziantoniouHighPerformanceRTLImplementation} for the near-lossless mode \cite{basconesRealTimeFPGAImplementation2022}. These results show that high throughput implementations of the issue 2 hybrid-entropy encoder are possible, outperforming those of issue 1 for the near-lossless mode. For the remainder of this document the focus will be on the predictor stage of the compressor.

The notation used in the following sections is the same as those used by the CCSDS specification in \cite{LowComplexityLosslessNearLossless2019}. Most notations will be mentioned in text, but not all are relevant for this discussion. For a full list, and the readers convenience, an overview is available in \autoref{tab:predictor-symbols}. Image dimensions are denoted as $N_z$, $N_x$ and $N_y$, where $z$ is the spectral band, and $x$ and $y$ the spatial dimensions. We will see that by carefully arranging the samples of the image before passing them to the compressor we can avoid some of the challenges in regards to hardware pipelining. For ease of use we introduce the notation $t$ for the current spatial sample position, leaving the choice of sample ordering out of the notation. Thus, sample $s_z(t)$ refers to the sample $s$ of spectral band $z$ and spatial position $t$.

\begin{table}[h]
	\caption{Issue 2 predictor symbol table.}
	\label{tab:predictor-symbols}
	\begin{center}
		\begin{tabular}[c]{l|l}
			\hline
			\multicolumn{1}{l|}{\textbf{Symbol}} &
			\multicolumn{1}{l}{\textbf{Name}}                                                      \\
			\hline
			$s_z(t)$                             & Image sample                                    \\
			$\sigma_z(t)$                        & Local sum                                       \\
			$U_z(t)$                             & Local difference vector                         \\
			$\hat{d}_z(t)$                       & Central local difference                        \\
			$\breve{s}_z(t)$                     & High-resolution predicted sample                \\
			$\tilde{s}_z(t)$                     & Double-resolution predicted sample              \\
			$\hat{s}_z(t)$                       & Predicted sample                                \\
			$\Delta_z(t)$                        & Prediction residual                             \\
			$q_z(t)$                             & Signed quantizer index                          \\
			$s^{'}_z(t)$                         & Clipped quantizer bin center                    \\
			$s^{''}_z(t)$                        & Sample representative                           \\
			$\theta_z(t)$                        & Predicted sample and sample endpoint difference \\
			$\delta_z(t)$                        & Mapped quantizer index                          \\
			$e_z(t)$                             & Double-resolution prediction error              \\
			$W_z(t)$                             & Weight vector                                   \\
			\hline
		\end{tabular}
	\end{center}
\end{table}


\begin{figure}[h]
	\begin{center}
		\includegraphics[width=0.8\textwidth]{figures/predictor-high-level.png}
	\end{center}
	\caption{Block diagram of the issue 2 predictor, recreated from \cite{hernandez-cabroneroCCSDS1230B2LowComplexity2021}. The new blocks as compared to issue 1 are marked in red. See \autoref{tab:predictor-symbols} for symbol names.}
	\label{fig:predictor-high-level}
\end{figure}

\autoref{fig:predictor-high-level} shows a block level view of the issue 2 predictor stage. To allow near-lossless compression, the issue 2 predictor introduces a quantization step, and sample representatives, marked in red. In the quantization step, the difference between input samples $s_z(t)$ and predicted sample values $\hat{s}_z(t)$, named prediction residuals $\Delta_z(t)$, are grouped into bins. Thus collecting nearby values, reducing the possible sample value ranges and allowing higher compression ratios. The signed quantizer indices $q_z(t)$ are mapped to unsigned indices, the mapped quantizer indices $\delta_z(t)$, which are passed on to the encoder. Because the real sample values are not available to the decoder, the predictor uses sample representatives $s^{''}_z(t)$ during sample prediction. The use of sample representatives is one of the main reasons for the challenges of implementing the algorithm for high throughput hardware applications \cite{sanchezReducingDataDependencies2022}.

\begin{figure}[h]
	\begin{center}
		\includegraphics[width=1\textwidth]{figures/predictor-data-deps.png}
	\end{center}
	\caption{Data dependencies of the issue 2 predictor stage, recreated from \cite{sanchezReducingDataDependencies2022}. See \autoref{tab:predictor-symbols} for symbol names.}
	\label{fig:predictor-data-deps}
\end{figure}

\autoref{fig:predictor-data-deps} shows the data dependencies of the issue 2 predictor, where the critical path for hardware implementations is marked with red arrows. The sample representatives of previous spatial and spectral samples $s^{''}_{\leq z}(<t)$, forming the prediction neighborhood, are used to calculate the local sum $\sigma_z(t)$ and local difference vector $U_z(t)$, the latter holding local sum differences for the current and spectrally neighbouring samples. A key element in the adaptive nature of the prediction method is the weight vector $W_z(t)$. The vector is updated for each iteration of the algorithm, and controls how the local differences of spectrally neighbouring samples are weighted when calculating the central local difference $\hat{d}_z(t)$ of the current sample. Weight updates depend on the results of the sample representatives, quantization step and prediction calculations, and must be completed before the next sample can be processed. As indicated by the red arrows, this creates a bottleneck for efficient hardware implementations.

\section{Hardware Implementations}

Several implementations of issue 1 exist, achieving high throughputs that reach the real-time requirements of hyperspectral image sensors, the highest of which reaching 1387$Msamples/s$ \cite{tsigkanosHighPerformanceCOTSFPGA2020}. The parallel FPGA implementation by \citeauthor{orlandicParallelFPGAImplementation2019} is in active use onboard the HYPSO satellites, with a throughput of 750$Msamples/s$ \cite{orlandicParallelFPGAImplementation2019, grotteOceanColorHyperspectral2022}. For HYPSO-1 images the compression ratio using issue 1 is about 50$\%$ reduction in image size \cite{vorhaugDevelopmentIntegrationCCSDS}. Issue 2 is capable of significantly higher compression ratios, depending on the error limit, motivating a switch over from issue 1.

The data dependencies of issue 2 introduced by the inclusion of the sample representatives creates significant challenges for hardware implementation. Here we make the distinction between spectral, $z-1$ dependencies, and spatial, $t-1$ dependencies, where the ordering of the input samples determines their impact. Three main orderings exist: band interleaved by pixel (BIP), band sequential (BSQ) and band interleaved by line (BIL), in which pixels are traversed in the coordinate orders (z, y, x), (x, y, z) and (x, z, y) respectively \cite{basconesRealTimeFPGAImplementation2022}. Choosing the BIP ordering results in a spectral dependency in the calculation of the local differences, which requires the local sum of the previous sample in the same band. By using the BSQ or BIL orderings, spectrally neighbouring samples are placed far enough apart in the processing order that this dependency has time to resolve. This, however, introduces a spatial dependency related to the weight updating scheme, which requires the sample representatives of spatially neighbouring samples (red path in \autoref{fig:predictor-data-deps}). Three main approaches have been proposed to alleviate these dependency problems, which will be discussed in the following sections.

The work by \citeauthor{barriosHardwareImplementationCCSDS2021} introduces the first implementation for FPGA of issue 2 in open literature \cite{barriosHardwareImplementationCCSDS2021}. This initial attempt followed a High-Level Synthesis (HLS) methodology, reaching a somewhat underwhelming throughput of 17.86 $Msamples/s$. The advantage of using the HLS approach is reduced development time, as the algorithm can be described in a high-level language like C or C++, which is then converted to the Register Transfer Level (RTL) \cite{barriosHardwareImplementationCCSDS2021}. However, the tools for such a conversion create suboptimal results when compared to writing the implementation directly in RTL using languages like VHDL or Systemverilog, giving the designer fine-grained control over optimizations. The design by \citeauthor{barriosHardwareImplementationCCSDS2021} does not make any special considerations as to the data dependencies of the algorithm, resulting in an almost fully serial processing of the samples and thus an unacceptably low throughput. The authors suggest using multiple compression units in parallel to increase total system performance. This is made possible by segmenting the image before compression, an approach successfully utilized by implementations of issue 1 \cite{tsigkanosHighPerformanceCOTSFPGA2020}.

\citeauthor{basconesRealTimeFPGAImplementation2022} presents the first viable real-time implementation. By introducing a novel sample ordering scheme, named frame interleaved by diagonal (FID), samples are placed far enough apart that the mentioned dependencies are avoided. This allows a deeply pipelined design, reaching a throughput of 285$Msamples/s$  \cite{basconesRealTimeFPGAImplementation2022} using VHDL RTL. The new sample ordering requires the use of internal reordering buffers at the beginning and end of the processing pipeline to convert from the conventional BIP ordering, adding complexity and logic resource usage.

\citeauthor{chatziantoniouScalableDataRateNearLossless2022a} identifies that the in-loop quantization step (\autoref{fig:predictor-high-level}) can be extracted and performed prior to the prediction stage \cite{chatziantoniouScalableDataRateNearLossless2022a}. This removes the need for sample representatives, effectively leaving a predictor similar to that of issue 1, void of the challenging dependencies. The single core throughput of this approach matches that of \citeauthor{basconesRealTimeFPGAImplementation2022}, and by use of segmentation and five cores they reach 1375$Msamples/s$. However, pre-quantization comes at the cost of a reduction in compression ratio as compared to that of the inline quantizer of the standard.

By using the FID ordering or pre-quantization in combination with image segmentation, high throughputs can be achieved. For systems with constraints on hardware logic resources, like the HYPSO mission, these approaches are not viable. Therefore, a different line of optimization must be pursued.

\begin{figure}[h]
	\begin{center}
		\includegraphics[width=0.8\textwidth]{figures/pipeline-math-opt.png}
	\end{center}
	\caption{Suggested pipeline stages for hardware implementation using the mathematical optimization proposed by \citeauthor{sanchezReducingDataDependencies2022}. Recreated from \cite{sanchezReducingDataDependencies2022}.}
	\label{fig:pipeline-math-opt}
\end{figure}

\citeauthor{sanchezReducingDataDependencies2022} identifies that by using a combination of the predictor configurations narrow local sums and reduced prediction mode, with the BIL ordering, most spatial dependencies on sample representatives are removed \cite{sanchezReducingDataDependencies2022}. These configurations select which neighbouring samples are to be used during the calculation of local sums and differences. Furthermore, they suggest a mathematically equivalent way of calculating the first step of the weight update, namely the double-resolution prediction error $e_z(t)$, removing its dependency on the quantization and sample representative steps (\autoref{fig:predictor-data-deps}). This, in combination with a speculative calculation of the possible outcomes, allows the weight update stage to be calculated in parallel with the prediction stage. As seen in the suggested pipeline shown in \autoref{fig:pipeline-math-opt}, this allows a significantly higher degree of pipelining of the design. This approach has been successfully implemented by \citeauthor{vorhaugDevelopmentIntegrationCCSDS} using VHDL RTL, achieving a throughput of 110$Msamples/s$ with a hardware logic utilization of roughly 1/3rd of that of the core proposed by \citeauthor{basconesRealTimeFPGAImplementation2022} in terms of LUTs, and 1/8th in terms of FFs \cite{vorhaugDevelopmentIntegrationCCSDS, basconesRealTimeFPGAImplementation2022}. This is the issue 2 compliant implementation with the lowest hardware footprint in open literature, and has been tested onboard the HYPSO-1 satellite, achieving a reduction in image size of 66$\%$ in the lossless compression mode.

By carefully tuning the user configurable parameters related to weight updates, \citeauthor{barriosRemovingDataDependencies2024} suggest that weight updates can be skipped entirely, allowing full pipelining \cite{barriosRemovingDataDependencies2024}. To avoid significant degradation in compression performance, careful consideration must be taken when selecting the initial weight vector for such a design. These initial values depend heavily on the image sensor and image objective, and thus the compressor is not able to run fully independently. Using the HLS methodology they reach a throughput of 235.4$Msamples/s$ with a somewhat lower hardware footprint than that of \citeauthor{basconesRealTimeFPGAImplementation2022}.

\begin{figure}[h]
	\begin{center}
		\includegraphics[width=0.5\textwidth]{figures/weight-updates.png}
	\end{center}
	\caption{Reduced weight updating frequency allows a shorter critical path in hardware implementations, yielding higher throughput. Recreated from \cite{jiaRemovalFeedbackLoop2025}.}
	\label{fig:weight-updates}
\end{figure}

The mathematical optimization approach suggested by \citeauthor{sanchezReducingDataDependencies2022} leaves the combined weight updating and prediction calculation stage as the critical path of the pipelined design (\autoref{fig:pipeline-math-opt}), constraining the obtainable maximum throughput \cite{sanchezReducingDataDependencies2022, jiaRemovalFeedbackLoop2025}. To alleviate this, \citeauthor{jiaRemovalFeedbackLoop2025} suggest selectively skipping the weight updates \cite{jiaRemovalFeedbackLoop2025}. This gives the weight update stage more time to complete, allowing a higher clock frequency at the cost of a slight degradation in compression performance. To combat this, the authors suggest a running correction to the applied weight updates which take the previous sample into consideration while keeping the new timing margins for the updates. \autoref{fig:weight-updates} shows a weight updating scheme with reduced frequency to every four samples. Using VHDL RTL their implementation of the issue 2 predictor reaches a throughput of 348.4$Msamples/s$, thus outperforming all the predictors discussed so far. The approach is also able to keep a small hardware footprint of only 3995 LUTs and 5050 FFs. For projects like HYPSO, the reduced weight updating frequency seems like a promising direction for the implementation of HSI compression. A full implementation, of both the predictor and encoder, should therefore be attempted.
